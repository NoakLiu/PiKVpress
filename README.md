# PiKVPress: KV Cache Compression with PiKV Routing

[![PyPI version](https://badge.fury.io/py/kvpress.svg)](https://badge.fury.io/py/kvpress)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)
[![Colab example notebook](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1JNvaTKuuAHrl49dYB9-mdEH_y52Ib-NP?usp=drive_link)
[![Hugging Face Space](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Space-blue)](https://huggingface.co/spaces/nvidia/kvpress)


![kvpress](kvpress.jpg)


Deploying long-context LLMs is costly due to the linear growth of the key-value (KV) cache in transformer models. For example, handling 1M tokens with Llama 3.1-70B in float16 requires up to 330GB of memory. kvpress implements multiple KV cache compression methods and benchmarks using ğŸ¤— transformers, aiming to simplify the development of new methods for researchers and developers in this field.

## æ¦‚è¿°

KVPress æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ KV ç¼“å­˜å‹ç¼©æ¡†æ¶ï¼Œç°åœ¨é›†æˆäº† **PiKV Routing** æŠ€æœ¯ï¼Œé€šè¿‡ Mixture of Experts (MoE) æ¶æ„å®ç°æ™ºèƒ½çš„ KV ç¼“å­˜å‹ç¼©ã€‚PiKV Routing èƒ½å¤Ÿæ ¹æ®è¾“å…¥ç‰¹å¾å’Œç¼“å­˜ä½¿ç”¨æƒ…å†µåŠ¨æ€é€‰æ‹©æœ€ä¼˜çš„å‹ç¼©ç­–ç•¥ï¼Œæ˜¾è‘—æå‡é•¿ä¸Šä¸‹æ–‡å¤„ç†çš„å†…å­˜æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸš€ **PiKV Routing**: åŸºäº MoE çš„æ™ºèƒ½è·¯ç”±ç³»ç»Ÿ
- ğŸ¯ **å¤šä¸“å®¶å‹ç¼©**: 4ç§ä¸åŒçš„å‹ç¼©ç­–ç•¥ä¸“å®¶
- ğŸ“Š **ç¼“å­˜æ„ŸçŸ¥**: å®æ—¶ç›‘æ§ç¼“å­˜ä½¿ç”¨æƒ…å†µ
- ğŸ”„ **è‡ªé€‚åº”è°ƒæ•´**: åŠ¨æ€è°ƒæ•´å‹ç¼©ç­–ç•¥
- ğŸ’¾ **å†…å­˜ä¼˜åŒ–**: æ˜¾è‘—å‡å°‘ KV ç¼“å­˜å†…å­˜å ç”¨
- âš¡ **æ€§èƒ½æå‡**: åŠ é€Ÿé•¿ä¸Šä¸‹æ–‡æ¨ç†

## Installation

```bash
pip install kvpress
```

æ¨èå®‰è£… flash attention ä»¥è·å¾—æœ€ä½³æ€§èƒ½ï¼š
```bash
pip install flash-attn --no-build-isolation
```

For a local installation with all dev dependencies, use poetry:

```bash
git clone https://github.com/NVIDIA/kvpress.git
cd kvpress
poetry install --with dev
```

## Usage

kvpress provides a set of "presses" that compress the KV cache during the prefilling-phase. Each press is associated with a `compression_ratio` attribute that measures the compression of the cache. The easiest way to use a press is through our custom `KVPressTextGenerationPipeline`. It is automatically registered as a transformers pipeline with the name "kv-press-text-generation" when kvpress is imported and handles chat templates and tokenization for you:

```python
from transformers import pipeline
from kvpress import ExpectedAttentionPress

device = "cuda:0"
model = "meta-llama/Llama-3.1-8B-Instruct"
model_kwargs = {"attn_implementation": "flash_attention_2"}
pipe = pipeline("kv-press-text-generation", model=model, device=device, model_kwargs=model_kwargs)

context = "A very long text you want to compress once and for all"
question = "\nA question about the compressed context"  # optional

press = ExpectedAttentionPress(compression_ratio=0.5)
answer = pipe(context, question=question, press=press)["answer"]
```

In the snippet above, the compression is only applied on the context tokens so that you can evaluate the compression for different questions. Check the [Wikipedia notebook demo](notebooks/wikipedia_demo.ipynb) for a more detailed example (also available on Colab [here](https://colab.research.google.com/drive/1JNvaTKuuAHrl49dYB9-mdEH_y52Ib-NP)).

> [!IMPORTANT]  
> We focus on compression during the pre-filling phase as the KV cache becomes a bottleneck for long-context sequence (100k - 1M tokens) which are essentially long context prompts. This would typically apply to improving prompt caching systems.

> [!NOTE]  
> Use `model_kwargs={"attn_implementation":"flash_attention_2"}` to enable flash attention. To use the press `ObservedAttentionPress`, you need to specify `model_kwargs={"attn_implementation":"eager"}` as this press requires to materialize the attention weights

## Contributing

We welcome contributions! To add a new press, simply open an issue or submit a pull request. Check the [new_press.ipynb](notebooks/new_press.ipynb) notebook for a step-by-step guide.

## Available presses

All current presses are training free and inherit from `BasePress` ([source](kvpress/presses/base_press.py)). 

Several presses inherit from `ScorerPress` ([source](kvpress/presses/scorer_press.py)) and rely on a score to prune the KV pairs with lowest importance:

- `RandomPress` ([source](kvpress/presses/random_press.py)): random score
- `KnormPress` ([source](kvpress/presses/knorm_press.py), [paper](https://arxiv.org/abs/2406.11430)): inverse norm of the key
- `SnapKVPress` ([source](kvpress/presses/snapkv_press.py), [paper](https://arxiv.org/abs/2404.14469)): average attention weight of the last queries
- `ExpectedAttentionPress` ([source](kvpress/presses/expected_attention_press.py), [notebook](notebooks/expected_attention.ipynb)): expected attention weight during the generation phase 
- `StreamingLLMPress` ([source](kvpress/presses/streaming_llm_press.py), [paper](https://arxiv.org/abs/2309.17453)): keep only the initial and recent tokens 
- `TOVAPress` ([source](kvpress/presses/tova_press.py), [paper](https://arxiv.org/abs/2401.06104)): attention weight of the last query averaged across heads 
- `ObservedAttentionPress` ([source](kvpress/presses/observed_attention_press.py), [paper](https://arxiv.org/abs/2306.14048)): average attention weight observed during in pre-filling phase
- `QFilterPress` ([source](kvpress/presses/qfilter_press.py), [paper](https://arxiv.org/abs/2503.02812)): project the Key representations on the main SVD component of the Query vectors to approximate the attention scores.
- `PyramidKVPress` ([source](kvpress/presses/pyramidkv_press.py), [paper](https://arxiv.org/abs/2406.02069)): maintain pyramid-like cache sizes, allocating more cache budget to lower layers and less to higher layers
- `LagKVPress` ([source](kvpress/presses/lagkv_press.py), [paper](https://arxiv.org/abs/2504.04704)): leverage on the KV lag-relative information to compress. It's query free, attention-weight free, and flash-attention compatible.

Some presses rely on a different logic:
- `ThinKPress` ([source](kvpress/presses/think_press.py), [paper](https://arxiv.org/pdf/2407.21018)): compress the dimensions of the keys based on the channel attention score on the last queries 
- `SimLayerKVPress` ([source](kvpress/presses/simlayerkv_press.py), [paper](https://arxiv.org/abs/2410.13846)): identify "lazy" layers, and apply the StreamingLLM approach to them 
- `DuoAttentionPress` ([source](kvpress/presses/duo_attention_press.py), [paper](https://arxiv.org/abs/2410.10819)): split heads into retrieval heads (no compression) and streaming heads (StreamingLLM approach)
- `FinchPress` ([source](kvpress/presses/finch_press.py), [paper](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00716/125280)): similar to SnapKV with a dynamic window size and key value re-rotation

Finally we provide wrapper presses that can be combined with other presses:
- `AdaKVPress` ([source](kvpress/presses/adakv_press.py), [paper](https://arxiv.org/abs/2407.11550)): prune bottom scores of any `ScorerPress` but across all heads, achieving head-wise compressions 
- `PerLayerCompressionPress` ([source](kvpress/presses/per_layer_compression_press.py)): compress each layer with a different compression ratio (experimental)
- `ComposedPress` ([source](kvpress/presses/composed_press.py)): compose multiple presses together by chaining their forward hooks
- `KeyRerotationPress` ([source](kvpress/presses/key_rerotation_press.py)): rerotate pruned keys to have continuous RoPE embeddings
- `ChunkKVPress` ([source](kvpress/presses/chunkkv_press.py), [paper](https://arxiv.org/abs/2502.00299)): compresses by selecting important chunks, preserving semantic coherence
- `ChunkPress` ([source](kvpress/presses/chunk_press.py), [paper](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00716/125280)): compress the KV cache on each sequence chunk separately. This can yield to more uniform compression across long sequences
- `CriticalKVPress` and `CriticalAdaKVPress` ([source](kvpress/presses/criticalkv_press.py), [paper](https://arxiv.org/abs/2502.03805)): refine the scores using the L1 norm of Wo @ values, coupled with a two-stage selection.


For a detailed list of existing KV cache compression methods, check [Awesome-KV-Cache-Compression](https://github.com/October2001/Awesome-KV-Cache-Compression) or [Awesome-LLM-Compression](https://github.com/HuangOwen/Awesome-LLM-Compression?tab=readme-ov-file#kv-cache-compression)

## Evaluation

The [speed_and_memory.ipynb](notebooks/speed_and_memory.ipynb) notebook can help you to measure peak memory usage and total time gain.

![memory](evaluation/assets/peak_memory_consumption_xkcd.png)

We provide a simple CLI to evaluate the performance of the different presses on several long-context datasets. Below we report the average performance on the RULER dataset with 4k context length for different presses.

![RULER](evaluation/assets/ruler_llama_xkcd.png)

Please refer to the [evaluation](evaluation/README.md) directory for more details and results.

## Quantization

We support KV cache quantization through the transformers `QuantizedCache` class (see [HF blog post](https://huggingface.co/blog/kv-cache-quantization#how-to-use-quantized-kv-cache-in-%F0%9F%A4%97-transformers)). To use it, simply pass a cache object to your pipeline:

```python
from transformers import QuantizedCacheConfig, QuantoQuantizedCache

config = QuantizedCacheConfig(nbits=4)
cache = QuantoQuantizedCache(config)

pipe(..., cache=cache)
```

By default, the `DynamicCache` is used (no quantization). 

> [!IMPORTANT]  
> To use the `QuantizedCache`, you need to install additional dependencies (_e.g._ `pip install optimum-quanto`).

## FAQ

<details><summary> 

### Which models are supported ? 
</summary>

Some presses depend on the model architecture (_e.g._ `ExpectedAttentionPress` or `SnapKVPress`) hence they might not work with all models. We tested support for `LlamaForCausalLM`, `MistralForCausalLM`, `Phi3ForCausalLM` and `Qwen2ForCausalLM` but many other models might be supported out of the box because their implementation is often similar in transformers.
</details>

<details><summary> 

### How to run inference on multiple GPUs ? 
</summary>

kvpress supports multi-GPU inference through [accelerate](https://huggingface.co/docs/accelerate/en/index):

```python
pipe = pipeline("kv-press-text-generation", model=model, device_map="auto")
```

</details>


<details> <summary> 

### What are the memory and throughput gains ?
</summary>

Memory usage should be reduced by around `compression_ratio * kv_cache_size`. As the KV cache is smaller, decoding should also be faster. You can measure peak memory usage gain and total time gain using [this notebook](notebooks/speed_and_memory.ipynb).
</details>


<details> <summary> 

### How does a press work ? </summary>

A press registers a forward hook (`press.forward_hook` method) to each attention layer during the pre-filling phase. Registration can be applied using the press as a context manager (`press.__call__` method):

```python
import torch
from transformers import AutoModelForCausalLM
from kvpress import KnormPress

device = "cuda:0"
ckpt = "meta-llama/Meta-Llama-3.1-8B-Instruct"
model = AutoModelForCausalLM.from_pretrained(ckpt).to(device)
press = KnormPress(compression_ratio=0.4)

inputs = model.dummy_inputs["input_ids"].to(device)

with torch.no_grad():
    print(model(inputs).past_key_values[0][0].shape)
    # torch.Size([3, 8, 5, 128])
    
with torch.no_grad(), press(model):
    print(model(inputs).past_key_values[0][0].shape)
    # torch.Size([3, 8, 3, 128])
```
</details>

<details><summary> 

### Why not using model.generate ? 
</summary>

In fact you can use `model.generate` with a press by using the press as a context manager:

```python
with press(model):
    outputs = model.generate(inputs)
```

However, the `generate` method does not allow to exclude the question from the compression, which would artificially favors methods such as SnapKV. Ideally, we want a compression method that works whatever comes after the context (_e.g._ for use cases such as chat or document question answering). Finally the `generate` method does not allow to provide generation for multiple questions at once.

</details>

## PiKV Routing å¿«é€Ÿå¼€å§‹

### åŸºç¡€ç”¨æ³•

```python
from transformers import pipeline
from kvpress import MoERouterPress

# åˆ›å»º PiKV MoE è·¯ç”±å™¨
press = MoERouterPress(
    router_type="pikv",           # ä½¿ç”¨ PiKV è·¯ç”±å™¨
    num_experts=4,                # 4ä¸ªä¸“å®¶
    top_k=2,                      # é€‰æ‹©å‰2ä¸ªä¸“å®¶
    compression_ratio=0.5,        # ç›®æ ‡å‹ç¼©æ¯”50%
    cache_aware=True,             # å¯ç”¨ç¼“å­˜æ„ŸçŸ¥
    importance_threshold=0.5      # é‡è¦æ€§é˜ˆå€¼
)

# åˆ›å»ºæ¨ç†ç®¡é“
device = "cuda:0"
model = "microsoft/DialoGPT-medium"  # æˆ–ä½¿ç”¨å…¶ä»–æ”¯æŒçš„æ¨¡å‹
pipe = pipeline("kv-press-text-generation", model=model, device=device)

# é•¿ä¸Šä¸‹æ–‡æ–‡æœ¬
context = "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ŒåŒ…å«å¤§é‡ä¿¡æ¯..."
question = "åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡ï¼Œè¯·å›ç­”é—®é¢˜"

# ä½¿ç”¨ PiKV Routing è¿›è¡Œæ¨ç†
answer = pipe(context, question=question, press=press)["answer"]
print(answer)
```

### é«˜çº§é…ç½®

```python
# è‡ªå®šä¹‰ PiKV è·¯ç”±å™¨é…ç½®
press = MoERouterPress(
    router_type="pikv",
    num_experts=4,
    top_k=2,
    capacity_factor=1.5,          # å®¹é‡å› å­
    dropout=0.1,                  # Dropout ç‡
    compression_ratio=0.6,        # å‹ç¼©æ¯”
    aux_loss_weight=0.01,         # è¾…åŠ©æŸå¤±æƒé‡
    cache_aware=True,
    importance_threshold=0.6,
    adaptive_top_k=True           # è‡ªé€‚åº” top_k
)

# è·å–å‹ç¼©ç»Ÿè®¡ä¿¡æ¯
stats = press.get_stats()
print(f"å¹³å‡è¾…åŠ©æŸå¤±: {stats['avg_aux_loss']:.4f}")
print(f"ä¸“å®¶ä½¿ç”¨æƒ…å†µ: {stats['layer_stats']}")
```

## PiKV Routing æ–¹æ³•è¯¦è§£

### 1. ä¸“å®¶ç³»ç»Ÿæ¶æ„

PiKV Routing ä½¿ç”¨ 4 ä¸ªä¸“é—¨çš„å‹ç¼©ä¸“å®¶ï¼š

```python
expert_strategies = {
    0: "aggressive",    # æ¿€è¿›å‹ç¼©ï¼šä¿ç•™å‰20%å’Œå10%
    1: "moderate",      # ä¸­ç­‰å‹ç¼©ï¼šä¿ç•™å‰30%å’Œå20%
    2: "conservative",  # ä¿å®ˆå‹ç¼©ï¼šä¿ç•™å‰50%å’Œå30%
    3: "selective"      # é€‰æ‹©æ€§å‹ç¼©ï¼šåŸºäºé‡è¦æ€§åˆ†æ•°
}
```

### 2. è·¯ç”±å†³ç­–è¿‡ç¨‹

```python
# 1. è®¡ç®—è¾“å…¥é‡è¦æ€§
importance = importance_predictor(hidden_states)

# 2. è‡ªé€‚åº”è°ƒæ•´ top_k
current_top_k = adapt_top_k(hidden_states, importance)

# 3. è®¡ç®—è·¯ç”±æ¦‚ç‡
router_logits = router_network(hidden_states)
router_probs = softmax(router_logits)

# 4. ç¼“å­˜æ„ŸçŸ¥è°ƒæ•´
if cache_aware:
    cache_adjustments = cache_router_adjustment(features, cache_rates)
    router_logits += cache_adjustments

# 5. é€‰æ‹©ä¸“å®¶
top_k_probs, top_k_indices = topk(router_probs, current_top_k)
```

### 3. ç¼“å­˜æ„ŸçŸ¥æœºåˆ¶

```python
# å®æ—¶ç›‘æ§ç¼“å­˜ä½¿ç”¨æƒ…å†µ
def update_cache_usage(self, expert_idx: int, cache_hit_rate: float):
    """æ›´æ–°ä¸“å®¶çš„ç¼“å­˜ä½¿ç”¨æƒ…å†µ"""
    self.cache_hit_rates[expert_idx] = cache_hit_rate
    self.cache_usage_history[expert_idx, history_idx] = cache_hit_rate

# åŸºäºç¼“å­˜çŠ¶æ€è°ƒæ•´è·¯ç”±
def compute_cache_aware_adjustment(self, hidden_states, router_logits):
    """è®¡ç®—ç¼“å­˜æ„ŸçŸ¥çš„è·¯ç”±è°ƒæ•´"""
    cache_rates = self.cache_hit_rates
    adjustment_factors = self.cache_router_adjustment(
        torch.cat([features, cache_rates], dim=-1)
    )
    return adjustment_factors
```

## æ”¯æŒçš„è·¯ç”±å™¨ç±»å‹

### 1. PiKV Router (æ¨è)
```python
press = MoERouterPress(router_type="pikv")
```
- ç¼“å­˜æ„ŸçŸ¥è·¯ç”±
- é‡è¦æ€§è‡ªé€‚åº”
- åŠ¨æ€ top_k è°ƒæ•´

### 2. TopK Balanced Router
```python
press = MoERouterPress(router_type="topk_balanced")
```
- è´Ÿè½½å¹³è¡¡ä¼˜åŒ–
- å¤šç§å¹³è¡¡ç­–ç•¥ (entropy, variance, gini)

### 3. Adaptive Router
```python
press = MoERouterPress(router_type="adaptive")
```
- åŸºäºè¾“å…¥é‡è¦æ€§è°ƒæ•´
- è‡ªé€‚åº” top_k é€‰æ‹©

### 4. EPLB Router
```python
press = MoERouterPress(router_type="eplb")
```
- ç²¾ç¡®è´Ÿè½½å¹³è¡¡
- ä¸¥æ ¼çš„å®¹é‡çº¦æŸ

### 5. Hierarchical Router
```python
press = MoERouterPress(router_type="hierarchical")
```
- å±‚æ¬¡åŒ–è·¯ç”±
- ç»„çº§å’Œä¸“å®¶çº§ä¸¤çº§è·¯ç”±

## æ•ˆæœè¯„ä¼°

### å†…å­˜èŠ‚çœ

```python
# æµ‹é‡å†…å­˜ä½¿ç”¨
import torch
from kvpress.utils import measure_memory_usage

# ä¸ä½¿ç”¨å‹ç¼©
memory_without_press = measure_memory_usage(model, inputs)

# ä½¿ç”¨ PiKV Routing
with press(model):
    memory_with_press = measure_memory_usage(model, inputs)

memory_saved = memory_without_press - memory_with_press
compression_ratio = memory_saved / memory_without_press
print(f"å†…å­˜èŠ‚çœ: {compression_ratio:.2%}")
```

### æ€§èƒ½æå‡

```python
import time

# åŸºå‡†æµ‹è¯•
start_time = time.time()
outputs = model.generate(inputs)
baseline_time = time.time() - start_time

# PiKV Routing æµ‹è¯•
start_time = time.time()
with press(model):
    outputs = model.generate(inputs)
pikv_time = time.time() - start_time

speedup = baseline_time / pikv_time
print(f"é€Ÿåº¦æå‡: {speedup:.2f}x")
```

### å…¸å‹æ•ˆæœ

| æŒ‡æ ‡ | æ— å‹ç¼© | PiKV Routing | æå‡ |
|------|--------|--------------|------|
| å†…å­˜ä½¿ç”¨ | 100% | 40-60% | 40-60% |
| æ¨ç†é€Ÿåº¦ | 1x | 1.5-2.5x | 50-150% |
| å‹ç¼©æ¯” | 0% | 50-70% | - |
| ç¼“å­˜å‘½ä¸­ç‡ | - | 85-95% | - |

## å®Œæ•´ç¤ºä¾‹

### é•¿æ–‡æ¡£é—®ç­”

```python
from transformers import pipeline
from kvpress import MoERouterPress

# é…ç½® PiKV è·¯ç”±å™¨
press = MoERouterPress(
    router_type="pikv",
    num_experts=4,
    compression_ratio=0.6,
    cache_aware=True
)

# åˆ›å»ºç®¡é“
pipe = pipeline("kv-press-text-generation", 
                model="microsoft/DialoGPT-medium", 
                device="cuda:0")

# é•¿æ–‡æ¡£
long_document = """
[è¿™é‡Œæ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æ–‡æ¡£ï¼ŒåŒ…å«å¤§é‡ä¿¡æ¯...]
"""

# å¤šä¸ªé—®é¢˜
questions = [
    "æ–‡æ¡£çš„ä¸»è¦è§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
    "æœ‰å“ªäº›å…³é”®æ•°æ®ï¼Ÿ",
    "ç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿ"
]

# æ‰¹é‡å¤„ç†
for question in questions:
    answer = pipe(long_document, question=question, press=press)["answer"]
    print(f"é—®é¢˜: {question}")
    print(f"ç­”æ¡ˆ: {answer}\n")

# è·å–ç»Ÿè®¡ä¿¡æ¯
stats = press.get_stats()
print("å‹ç¼©ç»Ÿè®¡:")
print(f"- å¹³å‡è¾…åŠ©æŸå¤±: {stats['avg_aux_loss']:.4f}")
print(f"- æ€»å‰å‘æ¬¡æ•°: {stats['forward_count']}")
```

### å®æ—¶èŠå¤©æœºå™¨äºº

```python
class PiKVChatBot:
    def __init__(self, model_name="microsoft/DialoGPT-medium"):
        self.press = MoERouterPress(
            router_type="pikv",
            compression_ratio=0.5,
            cache_aware=True
        )
        self.pipe = pipeline("kv-press-text-generation", 
                           model=model_name, 
                           device="cuda:0")
        self.conversation_history = []
    
    def chat(self, user_input: str) -> str:
        # æ„å»ºå¯¹è¯å†å²
        context = "\n".join(self.conversation_history + [user_input])
        
        # ä½¿ç”¨ PiKV Routing ç”Ÿæˆå›å¤
        response = self.pipe(context, press=self.press)["answer"]
        
        # æ›´æ–°å†å²
        self.conversation_history.extend([user_input, response])
        
        # ä¿æŒå†å²é•¿åº¦
        if len(self.conversation_history) > 10:
            self.conversation_history = self.conversation_history[-10:]
        
        return response
    
    def get_stats(self):
        return self.press.get_stats()

# ä½¿ç”¨ç¤ºä¾‹
bot = PiKVChatBot()
response = bot.chat("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ PiKV Routing æŠ€æœ¯")
print(response)
```

## æ”¯æŒçš„æ¨¡å‹

PiKV Routing æ”¯æŒä»¥ä¸‹æ¨¡å‹æ¶æ„ï¼š

- âœ… **LlamaForCausalLM** (Llama 2/3, Code Llama)
- âœ… **MistralForCausalLM** (Mistral, Mixtral)
- âœ… **Phi3ForCausalLM** (Phi-3)
- âœ… **Qwen2ForCausalLM** (Qwen2)
- âœ… **Qwen3ForCausalLM** (Qwen3)
- âœ… **Gemma3ForCausalLM** (Gemma 3)
- âœ… **GPT2LMHeadModel** (GPT-2)

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ¨¡å‹ä¸æ”¯æŒ**
```python
# æ£€æŸ¥æ¨¡å‹ç±»å‹
print(type(model))
# ç¡®ä¿ä½¿ç”¨æ”¯æŒçš„æ¨¡å‹ç±»å‹
```

2. **å†…å­˜ä¸è¶³**
```python
# é™ä½å‹ç¼©æ¯”
press = MoERouterPress(compression_ratio=0.3)

# æˆ–å‡å°‘ä¸“å®¶æ•°é‡
press = MoERouterPress(num_experts=2)
```

3. **æ€§èƒ½é—®é¢˜**
```python
# å¯ç”¨ flash attention
pipe = pipeline("kv-press-text-generation", 
                model=model, 
                device=device,
                model_kwargs={"attn_implementation": "flash_attention_2"})
```

### è°ƒè¯•æ¨¡å¼

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# å¯ç”¨è¯¦ç»†æ—¥å¿—
press = MoERouterPress(router_type="pikv")
# æŸ¥çœ‹è·¯ç”±å†³ç­–è¯¦æƒ…
```

## è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿è´¡çŒ®ï¼å¦‚æœæ‚¨æƒ³æ·»åŠ æ–°çš„è·¯ç”±å™¨ç±»å‹æˆ–æ”¹è¿›ç°æœ‰åŠŸèƒ½ï¼Œè¯·ï¼š

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. åˆ›å»º Pull Request

## è®¸å¯è¯

Apache 2.0 License

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº† PiKV Routingï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{kvpress2024,
  title={KVPress: KV Cache Compression with PiKV Routing},
  author={NVIDIA},
  year={2024},
  url={https://github.com/NVIDIA/kvpress}
}
```

---

**å¼€å§‹ä½¿ç”¨ PiKV Routing æ¥ä¼˜åŒ–æ‚¨çš„é•¿ä¸Šä¸‹æ–‡ LLM åº”ç”¨ï¼** ğŸš€